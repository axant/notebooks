
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MongoDB\n",
    "===========\n",
    "\n",
    " * Schema Free\n",
    " * Document Based\n",
    " * Supports Indexing\n",
    " * Not Transactional\n",
    " * Does not support relations (no JOIN)\n",
    " * Supports Autosharding\n",
    " * Automatic Replication and Failover\n",
    " * Relies on System Memory Manager\n",
    " * Has an Aggregation Pipeline\n",
    " * Builtin support for MapReduce\n",
    "\n",
    "On Python mongodb support is provided by ``PyMongo`` library, which can be installed using:\n",
    "\n",
    "````\n",
    "$ pip install pymongo\n",
    "```\n",
    "\n",
    "Installing MongoDB\n",
    "------------------\n",
    "\n",
    "Installing MongoDB is as simple as going to http://www.mongodb.org/downloads and downloading it.\n",
    "\n",
    "Create a ``/data/db`` directory then start ``mongod`` inside the mongodb downloaded package:\n",
    "\n",
    "```\n",
    "$ curl -O 'https://fastdl.mongodb.org/osx/mongodb-osx-x86_64-3.0.4.tgz' \n",
    "$ tar zxvf mongodb-osx-x86_64-3.0.4.tgz \n",
    "$ cd mongodb-osx-x86_64-3.0.4\n",
    "$ mkdir data\n",
    "$ ./bin/mongod --dbpath=./data\n",
    "```\n",
    "\n",
    "Using MongoDB\n",
    "----------------\n",
    "\n",
    "a ``MongoClient`` instance provides connection to MongoDB Server, each server can host multiple databases which can be retrieved with ``connection.database_name`` which can then contain multiple ``collections`` with different documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('mongodb://localhost:27017/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "db = client.phonebook\n",
    "print db.collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the database is retrieved, collections can be accessed as attributes of the database itself.\n",
    "\n",
    "A MongoDB document is actually just a Python Dictionary, inserting a document is as simple as telling pymongo to insert the dictionary into the collection. Each document can have its own structure, can contain different data and you are not required to declare and structure of the collection. Not existing collections will be automatically created on the insertion of the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectId('55893be77ab71c669f4c149f')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'name': 'Alessandro', 'phone': '+39123456789'}\n",
    "db.people.insert(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'people', u'system.indexes']\n"
     ]
    }
   ],
   "source": [
    "print db.collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each inserted document will receive an ``ObjectId`` which is a uniquue identifier of the document, the ObjectId is based on some data like the current ``timestamp``, ``server identifier`` ``process id`` and other data that guarantees it to be unique across multiple servers.\n",
    "\n",
    "Being designed to work in a distributed and multinode environment, MongoDB handles \"write safety\" by the number of servers that are expected to have saved the document before considering the insert command \"completed\".\n",
    "\n",
    "This is handled by the ``w`` option, which indicates the number of servers that must have saved the document before the insert command returns. Setting it to ``0`` makes mongodb work in *fire and forget* mode, which is useful when inserting a lot of documents quickly. As most drivers will actually generate the ObjectId on client that performs the insertion you will receive an ObjectId even before the document has been written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectId('55893a1d7ab71c669f4c149e')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.people.insert({'name': 'Puria', 'phone': '+39123456788', 'other_phone': '+3933332323'}, w=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot use 'w' > 1 when a host is not replicated\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    db.people.insert({'name': 'Puria', 'phone': '+39123456789'}, w=2)\n",
    "except Exception as e:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching back inserted document can be done using ``find`` and ``find_one`` methods of collections. Both methods accept a query expression that filters the returned documents. Omitting it means retrieving all the documents (or in case of find_one the first document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'_id': ObjectId('558939057ab71c669f4c149a'),\n",
       " u'name': u'Alessandro',\n",
       " u'phone': u'+39123456789'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.people.find_one({'name': 'Alessandro'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters in mongodb are described by Documents themselves, so in case of PyMongo they are dictionaries too.\n",
    "A filter can be specified in the form ``{'field': value}``. \n",
    "By default filtering is performed by *equality* comparison, this can be changed by specifying a query operator in place of the value.\n",
    "\n",
    "Query operators by convention start with a ``$`` sign and can be specified as ``{'field': {'operator': value}}``.\n",
    "Full list of query operators is available at http://docs.mongodb.org/manual/reference/operator/query/\n",
    "\n",
    "For example if we want to find each person that has an object id greather than ``53b30ff57ab71c051823b031`` we can achieve that with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'_id': ObjectId('55893be77ab71c669f4c149f'),\n",
       " u'name': u'Alessandro',\n",
       " u'phone': u'+39123456789'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bson import ObjectId\n",
    "db.people.find_one({'_id': {'$gt':  ObjectId('55893a1d7ab71c669f4c149e')}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating Documents\n",
    "---------------------\n",
    "\n",
    "Updating documents in MongoDB can be performed with the ``update`` method of the collection. Updating is actually one of the major sources of issues for new users as it doesn't change values in document like it does on SQL based databases, but instead it replaces the document with a new one.\n",
    "\n",
    "Also note that the update operation doesn't perform update on each document identified by the query, by default only the first document is updated. To apply it to multiple documents it is required to explicitly specify the ``multi=true`` option\n",
    "\n",
    "What you usually want to do is actually using the ``$set`` operator which changes the existing document instead of replacing it with a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before Updated: {u'phone': u'+39123456789', u'_id': ObjectId('53b30ff57ab71c051823b031'), u'name': u'Alessandro'}\n",
      "\n",
      "After Update: {u'_id': ObjectId('53b30ff57ab71c051823b031'), u'name': u'John Doe'}\n",
      "\n",
      "After $set phone: {u'phone': u'+39123456789', u'_id': ObjectId('53b30ff57ab71c051823b031'), u'name': u'John Doe'}\n",
      "\n",
      "After $set name: {u'phone': u'+39123456789', u'_id': ObjectId('53b30ff57ab71c051823b031'), u'name': u'Alessandro'}\n"
     ]
    }
   ],
   "source": [
    "doc = db.people.find_one({'name': 'Alessandro'})\n",
    "print '\\nBefore Updated:', doc\n",
    "\n",
    "db.people.update({'name': 'Alessandro'}, {'name': 'John Doe'})\n",
    "doc = db.people.find_one({'name': 'John Doe'})\n",
    "print '\\nAfter Update:', doc\n",
    "\n",
    "# Go back to previous state\n",
    "db.people.update({'name': 'John Doe'}, {'$set': {'phone': '+39123456789'}})\n",
    "print '\\nAfter $set phone:', db.people.find_one({'name': 'John Doe'})\n",
    "db.people.update({'name': 'John Doe'}, {'$set': {'name': 'Alessandro'}})\n",
    "print '\\nAfter $set name:', db.people.find_one({'name': 'Alessandro'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SubDocuments\n",
    "--------------\n",
    "\n",
    "The real power of mongodb is released when you use subdocuments.\n",
    "\n",
    "As each mongodb document is a JSON object (actually BSON, but that doesn't change much for the user), it can contain any data which is valid in JSON. Including other documents and arrays. This replaces \"relations\" between collections in multiple use cases and it's heavily more efficient as it returns all the data in a single query instead of having to perform multiple queries to retrieve related data.\n",
    "\n",
    "As MongoDB fully supports subdocuments it is also possible to query on sub document fields and even query on arrays using the ``dot notation``.\n",
    "\n",
    "For example if you want to store a blog post in mongodb you might actually store everything, including author data and tags inside the blogpost itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectId('55893ee57ab71c669f4c14a0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.blog.insert({'title': 'MongoDB is great!',\n",
    "                'author': {'name': 'Alessandro',\n",
    "                           'surname': 'Molina',\n",
    "                           'avatar': 'http://www.gravatar.com/avatar/7a952cebb086d2114080b4b39ed83cad.png'},\n",
    "                'tags': ['mongodb', 'web', 'scaling']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'_id': ObjectId('53b324d67ab71c051823b035'),\n",
       " u'author': {u'avatar': u'http://www.gravatar.com/avatar/7a952cebb086d2114080b4b39ed83cad.png',\n",
       "  u'name': u'Alessandro',\n",
       "  u'surname': u'Molina'},\n",
       " u'tags': [u'mongodb', u'web', u'scaling'],\n",
       " u'title': u'MongoDB is great!'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.blog.find_one({'title': 'MongoDB is great!'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'_id': ObjectId('55893ee57ab71c669f4c14a0'),\n",
       " u'author': {u'avatar': u'http://www.gravatar.com/avatar/7a952cebb086d2114080b4b39ed83cad.png',\n",
       "  u'name': u'Alessandro',\n",
       "  u'surname': u'Molina'},\n",
       " u'tags': [u'mongodb', u'web', u'scaling'],\n",
       " u'title': u'MongoDB is great!'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.blog.find_one({'tags': 'mongodb'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'_id': ObjectId('53b324d67ab71c051823b035'),\n",
       " u'author': {u'avatar': u'http://www.gravatar.com/avatar/7a952cebb086d2114080b4b39ed83cad.png',\n",
       "  u'name': u'Alessandro',\n",
       "  u'surname': u'Molina'},\n",
       " u'tags': [u'mongodb', u'web', u'scaling'],\n",
       " u'title': u'MongoDB is great!'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.blog.find_one({'author.name': 'Alessandro'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TAGS = ['mongodb', 'web', 'scaling', 'cooking']\n",
    "\n",
    "import random\n",
    "for postnum in range(1, 5):\n",
    "    db.blog.insert({'title': 'Post %s' % postnum,\n",
    "                    'author': {'name': 'Alessandro',\n",
    "                               'surname': 'Molina',\n",
    "                               'avatar': 'http://www.gravatar.com/avatar/7a952cebb086d2114080b4b39ed83cad.png'},\n",
    "                    'tags': random.sample(TAGS, 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB is great! -> mongodb, web, scaling\n",
      "Post 1 -> mongodb, cooking\n",
      "Post 2 -> cooking, web\n",
      "Post 3 -> web, cooking\n",
      "Post 4 -> cooking, mongodb\n"
     ]
    }
   ],
   "source": [
    "for post in db.blog.find({'tags': {'$in': ['scaling', 'cooking']}}):\n",
    "    print post['title'], '->', ', '.join(post['tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing\n",
    "----------\n",
    "\n",
    "Indexing is actually the most important part of MongoDB.\n",
    "\n",
    "MongoDB has great support for indexing, and it supports single key, multi key, compound and hashed indexes. Each index type has its specific use case and can be used both for querying and sorting.\n",
    "\n",
    " * Single Key -> Those are plain indexes on a field\n",
    " * Multi Key -> Those are indexes created on an array field\n",
    " * Compound -> Those are indexes that cover more than one field.\n",
    " * Hashed -> Those are indexes optimized for equality comparison, they actually store the hash of the indexed value and are usually used for sharding.\n",
    " \n",
    "In case of compound indexes they can also be used when only a part of the query filter is present into the index, there is also a special case of indexes called *covering indexes* which happen when the fields you are asking for are all available into the index. In that case MongoDB won't even access the collection and will directly serve you the data from the index. An index cannot be both a multi key index and a covering index.\n",
    "\n",
    "Indexes are also ordered, so they can be created *ASCENDING* or *DESCENDING*.\n",
    "\n",
    "Creating indexes can be done using the ``ensure_index`` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'tags_1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.blog.ensure_index([('tags', 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking which index MongoDB is using to perform a query can be done using the ``explain`` method, forcing an index into a query can be done using the ``hint`` method.\n",
    "\n",
    "As MongoDB uses a statistical optimizer, using ``hint`` in queries can actually provide a performance boost as it avoids the \"best option\" lookup cost of the optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'inputStage': {u'direction': u'forward',\n",
       "  u'indexBounds': {u'tags': [u'[\"mongodb\", \"mongodb\"]']},\n",
       "  u'indexName': u'tags_1',\n",
       "  u'isMultiKey': True,\n",
       "  u'keyPattern': {u'tags': 1},\n",
       "  u'stage': u'IXSCAN'},\n",
       " u'stage': u'FETCH'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.blog.find({'tags': 'mongodb'}).explain()['queryPlanner']['winningPlan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'filter': {u'tags': {u'$eq': u'mongodb'}},\n",
       " u'inputStage': {u'direction': u'forward',\n",
       "  u'indexBounds': {u'_id': [u'[MinKey, MaxKey]']},\n",
       "  u'indexName': u'_id_',\n",
       "  u'isMultiKey': False,\n",
       "  u'keyPattern': {u'_id': 1},\n",
       "  u'stage': u'IXSCAN'},\n",
       " u'stage': u'FETCH'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.blog.find({'tags': 'mongodb'}).hint([('_id', 1)]).explain()['queryPlanner']['winningPlan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'direction': u'forward',\n",
       " u'filter': {u'title': {u'$eq': u'Post 1'}},\n",
       " u'stage': u'COLLSCAN'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.blog.find({'title': 'Post 1'}).explain()['queryPlanner']['winningPlan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'inputStage': {u'direction': u'forward',\n",
       "  u'indexBounds': {u'author.name': [u'[\"Alessandro\", \"Alessandro\"]'],\n",
       "   u'title': [u'[MinKey, MaxKey]']},\n",
       "  u'indexName': u'author.name_1_title_1',\n",
       "  u'isMultiKey': False,\n",
       "  u'keyPattern': {u'author.name': 1, u'title': 1},\n",
       "  u'stage': u'IXSCAN'},\n",
       " u'stage': u'PROJECTION',\n",
       " u'transformBy': {u'_id': False, u'title': True}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.blog.ensure_index([('author.name', 1), ('title', 1)])\n",
    "db.blog.find({'author.name': 'Alessandro'}, {'title': True, '_id': False}).explain()['queryPlanner']['winningPlan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregation Pipeline\n",
    "----------------------\n",
    "\n",
    "The aggreation pipeline provided by the aggreation framework is a powerful feature in MongoDB that permits to perform complex data analysis by passing the documents through a pipeline of operations.\n",
    "\n",
    "MongoDB was created with the cover philosophy that you are going to store your documents depending on the way you are going to read them. So to properly design your schema you need to know how you are going to use the documents. While this approach provides great performance benefits and is more concrete in case of web application, it might not always be feasible.\n",
    "\n",
    "In case you need to perform some kind of analysis your documents are not optimized for, you can rely on the aggreation framework to create a pipeline that transforms them in a way more practical for the kind of analysis you need.\n",
    "\n",
    "### How it works\n",
    "\n",
    "The aggregation pipeline is a list of operations that gets executed one after the other on the documents of the collections. The first operation will be performed on all the documents, while successive operations are performed on the result of the previous steps.\n",
    "\n",
    "If steps are able to take advantage of **indexes** they will, that is the case for a **match** or **sort** operator, if it appears at the begin of the pipeline. All operators start with a <span><strong>$</strong></span> sign\n",
    "\n",
    "### Stage Operators\n",
    "\n",
    "\n",
    "* **project**\tReshapes each document in the stream, such as by adding new fields or removing existing fields. For each input document, outputs one document.\n",
    "* **match**\tFilters the document stream to allow only matching documents to pass unmodified into the next pipeline stage. **match** uses standard MongoDB queries. For each input document, outputs either one document (a match) or zero documents (no match).\n",
    "* **limit**\tPasses the first n documents unmodified to the pipeline where n is the specified limit. For each input document, outputs either one document (for the first n documents) or zero documents (after the first n documents).\n",
    "* **skip**\tSkips the first n documents where n is the specified skip number and passes the remaining documents unmodified to the pipeline. For each input document, outputs either zero documents (for the first n documents) or one document (if after the first n documents).\n",
    "* **unwind**\tDeconstructs an array field from the input documents to output a document for each element. Each output document replaces the array with an element value. For each input document, outputs n documents where n is the number of array elements and can be zero for an empty array.\n",
    "* **group**\tGroups input documents by a specified identifier expression and applies the accumulator expression(s), if specified, to each group. Consumes all input documents and outputs one document per each distinct group. The output documents only contain the identifier field and, if specified, accumulated fields.\n",
    "* **sort**\tReorders the document stream by a specified sort key. Only the order changes; the documents remain unmodified. For each input document, outputs one document.\n",
    "* **geoNear**\tReturns an ordered stream of documents based on the proximity to a geospatial point. Incorporates the functionality of **match**, **sort**, and **limit** for geospatial data. The output documents include an additional distance field and can include a location identifier field.\n",
    "* **out**\tWrites the resulting documents of the aggregation pipeline to a collection. To use the $out stage, it must be the last stage in the pipeline.\n",
    "\n",
    "#### Expression Operators\n",
    "\n",
    "Each stage operator can work with one or more **expression operator** which allow to perform actions during that stage, for a list of expression operators see http://docs.mongodb.org/manual/reference/operator/aggregation/#expression-operators\n",
    "\n",
    "### Pipeline Examples\n",
    "\n",
    "Examples are based on twitter database from the same S3 bucket used in **MrJob** examples imported in mongodb using:\n",
    "\n",
    "```\n",
    "$ curl -O http://panisson-bigdive.s3.amazonaws.com/twitter/2011-02-11/2011-02-11.json.aa.gz\n",
    "$ gunzip 2011-02-11.json.aa.gz\n",
    "$ mongoimport --db twitter --collection tweets 2011-02-11.json.aa\n",
    "2015-06-21T17:18:06.908+0200    connected to: localhost\n",
    "2015-06-21T17:18:09.896+0200    [#########...............] twitter.tweets       19.6 MB/50.0 MB (39.3%)\n",
    "2015-06-21T17:18:12.900+0200    [###################.....] twitter.tweets       41.1 MB/50.0 MB (82.2%)\n",
    "2015-06-21T17:18:13.720+0200    imported 20000 documents\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "[{u'count': 24, u'_id': u'golmadrid'}, {u'count': 28, u'_id': u'nowplaying'}, {u'count': 14, u'_id': u'ahoetshirtwouldsay'}, {u'count': 12, u'_id': u'GolMadrid'}, {u'count': 19, u'_id': u'RT'}, {u'count': 29, u'_id': u'Egypt'}, {u'count': 13, u'_id': u'cigarrasclipe'}, {u'count': 17, u'_id': u'jan25'}, {u'count': 41, u'_id': u'ipod'}, {u'count': 11, u'_id': u'NeverSayNever3D'}, {u'count': 18, u'_id': u'KevinJForKCA2011'}, {u'count': 11, u'_id': u'DamnItsTrue'}, {u'count': 11, u'_id': u'MuzikRadio'}, {u'count': 11, u'_id': u'fb'}, {u'count': 13, u'_id': u'1day'}, {u'count': 15, u'_id': u'followmejp'}, {u'count': 11, u'_id': u'TeamGetzItOut'}, {u'count': 12, u'_id': u'TeamFollowBack'}, {u'count': 13, u'_id': u'FF'}, {u'count': 15, u'_id': u'LiaRainhadoBloconoBBB'}, {u'count': 19, u'_id': u'sougofollow'}, {u'count': 12, u'_id': u'kevinjforkca2011'}, {u'count': 24, u'_id': u'AprendiComKpop'}, {u'count': 28, u'_id': u'np'}, {u'count': 11, u'_id': u'followme'}, {u'count': 13, u'_id': u'jobs'}]\n"
     ]
    }
   ],
   "source": [
    "db = client.twitter\n",
    "\n",
    "# How many professors wrote a tweet?\n",
    "print len(list(db.tweets.aggregate([\n",
    "    {'$match': {'user.description': {'$regex': 'Professor'}}}\n",
    "])))\n",
    "\n",
    "# Count them using only the pipeline\n",
    "print db.tweets.aggregate([\n",
    "    {'$match': {'user.description': {'$regex': 'Professor'}}},\n",
    "    {'$group': {'_id': 'count', 'count': {'$sum': 1}}}\n",
    "]).next()['count']\n",
    "\n",
    "\n",
    "# Hashtags frequency\n",
    "print list(db.tweets.aggregate([\n",
    "    {'$project': {'tags': '$entities.hashtags.text', '_id': 0}},\n",
    "    {'$unwind': '$tags'},\n",
    "    {'$group': {'_id': '$tags', 'count': {'$sum': 1}}},\n",
    "    {'$match': {'count': {'$gt': 20}}}\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapReduce\n",
    "----------\n",
    "\n",
    "MongoDB is powered by the V8 javascript engine, this means that each mongod node is able to run javascript code.\n",
    "With an high enough number of mongod nodes, you actually end up with a powerful execution environment for distributed code that also copes with the major problem of data locality.\n",
    "\n",
    "For this reason MongoDB exposes a **mapreduce** function which can be leveraged in shareded environments to run map reduce jobs.\n",
    "Note that the Aggregation Pipeline is usually faster than the mapReduce feature, and it scales with the number of nodes as mapReduce, so you should rely on MapReduce only when the algorithm cannot be efficiently expressed with the Aggregation Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'1day', u'value': 13.0}, {u'_id': u'AprendiComKpop', u'value': 24.0}, {u'_id': u'DamnItsTrue', u'value': 11.0}, {u'_id': u'Egypt', u'value': 29.0}, {u'_id': u'FF', u'value': 13.0}, {u'_id': u'GolMadrid', u'value': 12.0}, {u'_id': u'KevinJForKCA2011', u'value': 18.0}, {u'_id': u'LiaRainhadoBloconoBBB', u'value': 15.0}, {u'_id': u'MuzikRadio', u'value': 11.0}, {u'_id': u'NeverSayNever3D', u'value': 11.0}, {u'_id': u'RT', u'value': 19.0}, {u'_id': u'TeamFollowBack', u'value': 12.0}, {u'_id': u'TeamGetzItOut', u'value': 11.0}, {u'_id': u'ahoetshirtwouldsay', u'value': 14.0}, {u'_id': u'cigarrasclipe', u'value': 13.0}, {u'_id': u'fb', u'value': 11.0}, {u'_id': u'followme', u'value': 11.0}, {u'_id': u'followmejp', u'value': 15.0}, {u'_id': u'golmadrid', u'value': 24.0}, {u'_id': u'ipod', u'value': 41.0}, {u'_id': u'jan25', u'value': 17.0}, {u'_id': u'jobs', u'value': 13.0}, {u'_id': u'kevinjforkca2011', u'value': 12.0}, {u'_id': u'nowplaying', u'value': 28.0}, {u'_id': u'np', u'value': 28.0}, {u'_id': u'sougofollow', u'value': 19.0}]\n"
     ]
    }
   ],
   "source": [
    "db.tweets.map_reduce(\n",
    "    map='''function() {\n",
    "        var tags = this.entities.hashtags;\n",
    "        for(var i=0; i<tags.length; i++)\n",
    "            emit(tags[i].text, 1);\n",
    "    }''',\n",
    "    reduce='''function(key, values) {\n",
    "        return Array.sum(values); \n",
    "    }''',\n",
    "    out='tagsfrequency'\n",
    ")\n",
    "\n",
    "print(list(\n",
    "    db.tagsfrequency.find({'value': {'$gt': 10}})\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting from MongoDB\n",
    "-------------------------\n",
    "\n",
    "There are cases you might want to export the results of a mongodb query to make possible to process them into another system, this might be the case for an EMR job which has to perform operations on data stored on MongoDB.\n",
    "\n",
    "The most simple solution to export those data in a format recognized by EMR and MrJob is using the ``mongoexport`` tool provided with mongodb itself. The tool is able to export data in a format recognized by MrJob ``JSONValueProcotol`` so you can upload it to S3 and directly process it from EMR.\n",
    "\n",
    "For example, exporting all the data for the *web* tag, can be easily done using:\n",
    "\n",
    "    $ ./mongoexport -d phonebook -c blog -o /tmp/data.json -q '{\"tags\": \"web\"}'\n",
    "    \n",
    "This will write the data to */tmp/data.json* in a format recognized by ``JSONValueProtocol``, full list of options can be seen using ``--help``, in the previous example the following options were used:\n",
    "\n",
    " * -d -> to specify the database\n",
    " * -c -> to specify the collection in the database\n",
    " * -o -> to write output to /tmp/data.json\n",
    " * -q -> to filter output by the provided query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharding\n",
    "=====\n",
    "\n",
    "Sharding, or horizontal scaling, divides the data set and distributes the data over multiple servers, or shards. Each shard is an independent database, and collectively, the shards make up a single logical database.\n",
    "\n",
    "**Chunk**\n",
    "The whole set of data is divided in Chunks, chunk are then distributed as equally as possible through all the nodes\n",
    "\n",
    "**Shard Key**\n",
    "The shard key is the *Document* property on which chunks are decided, the range of shard key possible values is divided in chunks and each chunk is assigned to a node. Document which near values for the shard key will end up being in the same chunk and so on the same node.\n",
    "\n",
    "**Shard**\n",
    "Each MongoDB node or ReplicaSet that contains part of the sharded data.\n",
    "\n",
    "**Router**\n",
    "The routers is the interface to the cluster, each query and operation will be performed against the router. The router is then in charge of forwarding the operation to one or multiple shards and gather the results.\n",
    "\n",
    "**Config Server**\n",
    "The config servers keep track of chunks distribution, they know which shard contains which chunk and which values are kept inside each chunk. Whenever the router has to perform an operation or split chunks that became too big it will read and write chunks distribution from the config servers.\n",
    "\n",
    "Setting Up a Sharded Cluster\n",
    "---------------------------\n",
    "\n",
    "To properly setup a sharded environment at least **1 mongos**, **2 shards** and **1 config server** are required. That's the minimum requirement for a test environment and is not suitable for production usage.\n",
    "\n",
    "First we need to create the directories for each node:\n",
    "```\n",
    "$ mkdir /tmp/mongocluster\n",
    "$ mkdir /tmp/mongocluster/n0\n",
    "$ mkdir /tmp/mongocluster/n1\n",
    "$ mkdir /tmp/mongocluster/n2\n",
    "$ mkdir /tmp/mongocluster/c0\n",
    "\n",
    "```\n",
    "\n",
    "Then we need to start the shards:\n",
    "```\n",
    "$ mongod --port 27016 --dbpath /tmp/mongocluster/n0\n",
    "$ mongod --port 27015 --dbpath /tmp/mongocluster/n1\n",
    "$ mongod --port 27014 --dbpath /tmp/mongocluster/n2\n",
    "```\n",
    "\n",
    "Then we need to start at least a config server:\n",
    "```\n",
    "$ mongod --configsvr --dbpath /tmp/mongocluster/c0 --port 27019\n",
    "```\n",
    "\n",
    "Now that all the required nodes are up, we can finaly start the **mongos** router which is in charge of actually providing the sharding functionality:\n",
    "\n",
    "```\n",
    "$ mongos --configdb 127.0.0.1:27019\n",
    "```\n",
    "\n",
    "Now all the required nodes are up and running, but we still didn't configure any sharded environment.\n",
    "The first step required to setup a sharding environment is to actually add the nodes to the cluster.\n",
    "To do so we need to connect to the ``mongos`` and issue the ``sh.addShard`` command:\n",
    "\n",
    "```\n",
    "$ mongo \n",
    "MongoDB shell version: 3.0.4\n",
    "connecting to: test\n",
    "mongos> sh.addShard('127.0.0.1:27016')\n",
    "{ \"shardAdded\" : \"shard0000\", \"ok\" : 1 }\n",
    "mongos> sh.addShard('127.0.0.1:27015')\n",
    "{ \"shardAdded\" : \"shard0001\", \"ok\" : 1 }\n",
    "mongos> sh.addShard('127.0.0.1:27014')\n",
    "{ \"shardAdded\" : \"shard0002\", \"ok\" : 1 }\n",
    "``` \n",
    "\n",
    "Now that our shards have been added to the cluster we can turn on sharding for **databases** and **collections**.\n",
    "Only sharded collections will actually be sharded across the nodes.\n",
    "We are going to shard our collection of tweets, so the first step is to enable sharding for the ``twitter`` database:\n",
    "\n",
    "```\n",
    "mongos> sh.enableSharding('twitter')\n",
    "{ \"ok\" : 1 }\n",
    "```\n",
    "\n",
    "Now we need to provide the actual sharding key for our ``tweets`` collection. Until a sharding key is provided, no sharding happens. To ensure that tweets are properly distributed across nodes we are going to shard by the screen name of the author:\n",
    "\n",
    "```\n",
    "mongos> sh.shardCollection(\"twitter.tweets\", {'user.screen_name': 1})\n",
    "{ \"collectionsharded\" : \"twitter.tweets\", \"ok\" : 1 }\n",
    "```\n",
    "\n",
    "Now we can finally import our data and see that it gets distributed across the nodes:\n",
    "\n",
    "```\n",
    "$ mongoimport --db twitter --collection tweets 2011-02-11.json.aa\n",
    "```\n",
    "\n",
    "To check that our data has properly distributed across nodes:\n",
    "\n",
    "```\n",
    "mongos> use twitter\n",
    "switched to db twitter\n",
    "mongos> db.printShardingStatus()\n",
    "--- Sharding Status --- \n",
    "  sharding version: {\n",
    "\t\"_id\" : 1,\n",
    "\t\"minCompatibleVersion\" : 5,\n",
    "\t\"currentVersion\" : 6,\n",
    "\t\"clusterId\" : ObjectId(\"558fc718712e65efc2a378d9\")\n",
    "}\n",
    "  shards:\n",
    "\t{  \"_id\" : \"shard0000\",  \"host\" : \"localhost:27016\" }\n",
    "\t{  \"_id\" : \"shard0001\",  \"host\" : \"localhost:27015\" }\n",
    "\t{  \"_id\" : \"shard0002\",  \"host\" : \"localhost:27014\" }\n",
    "  balancer:\n",
    "\tCurrently enabled:  yes\n",
    "\tCurrently running:  no\n",
    "\tFailed balancer rounds in last 5 attempts:  0\n",
    "\tMigration Results for the last 24 hours: \n",
    "\t\t4 : Success\n",
    "  databases:\n",
    "\t{  \"_id\" : \"admin\",  \"partitioned\" : false,  \"primary\" : \"config\" }\n",
    "\t{  \"_id\" : \"test\",  \"partitioned\" : false,  \"primary\" : \"shard0002\" }\n",
    "\t{  \"_id\" : \"twitter\",  \"partitioned\" : true,  \"primary\" : \"shard0002\" }\n",
    "\t\ttwitter.tweets\n",
    "\t\t\tshard key: { \"user.screen_name\" : 1 }\n",
    "\t\t\tchunks:\n",
    "\t\t\t\tshard0000\t2\n",
    "\t\t\t\tshard0001\t2\n",
    "\t\t\t\tshard0002\t2\n",
    "\t\t\t{ \"user.screen_name\" : { \"$minKey\" : 1 } } -->> { \"user.screen_name\" : \"111111121111111\" } on : shard0000 Timestamp(2, 0) \n",
    "\t\t\t{ \"user.screen_name\" : \"111111121111111\" } -->> { \"user.screen_name\" : \"YohannaCS\" } on : shard0001 Timestamp(3, 0) \n",
    "\t\t\t{ \"user.screen_name\" : \"YohannaCS\" } -->> { \"user.screen_name\" : \"graciadelcielo\" } on : shard0000 Timestamp(4, 0) \n",
    "\t\t\t{ \"user.screen_name\" : \"graciadelcielo\" } -->> { \"user.screen_name\" : \"nosso_surita\" } on : shard0001 Timestamp(5, 0) \n",
    "\t\t\t{ \"user.screen_name\" : \"nosso_surita\" } -->> { \"user.screen_name\" : \"yuuki_gei\" } on : shard0002 Timestamp(5, 1) \n",
    "\t\t\t{ \"user.screen_name\" : \"yuuki_gei\" } -->> { \"user.screen_name\" : { \"$maxKey\" : 1 } } on : shard0002 Timestamp(1, 6) \n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**NOTE:** Splitting by a date or sequential values is usually not a good idea, as you end up enforcing all the workload on the primary node that contains the most recent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'1day', u'value': 13.0}, {u'_id': u'AprendiComKpop', u'value': 24.0}, {u'_id': u'DamnItsTrue', u'value': 11.0}, {u'_id': u'Egypt', u'value': 29.0}, {u'_id': u'FF', u'value': 13.0}, {u'_id': u'GolMadrid', u'value': 12.0}, {u'_id': u'KevinJForKCA2011', u'value': 18.0}, {u'_id': u'LiaRainhadoBloconoBBB', u'value': 15.0}, {u'_id': u'MuzikRadio', u'value': 11.0}, {u'_id': u'NeverSayNever3D', u'value': 11.0}, {u'_id': u'RT', u'value': 19.0}, {u'_id': u'TeamFollowBack', u'value': 12.0}, {u'_id': u'TeamGetzItOut', u'value': 11.0}, {u'_id': u'ahoetshirtwouldsay', u'value': 14.0}, {u'_id': u'cigarrasclipe', u'value': 13.0}, {u'_id': u'fb', u'value': 11.0}, {u'_id': u'followme', u'value': 11.0}, {u'_id': u'followmejp', u'value': 15.0}, {u'_id': u'golmadrid', u'value': 24.0}, {u'_id': u'ipod', u'value': 41.0}, {u'_id': u'jan25', u'value': 17.0}, {u'_id': u'jobs', u'value': 13.0}, {u'_id': u'kevinjforkca2011', u'value': 12.0}, {u'_id': u'nowplaying', u'value': 28.0}, {u'_id': u'np', u'value': 28.0}, {u'_id': u'sougofollow', u'value': 19.0}]\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client.twitter\n",
    "\n",
    "db.tweets.map_reduce(\n",
    "    map='''function() {\n",
    "        var tags = this.entities.hashtags;\n",
    "        for(var i=0; i<tags.length; i++)\n",
    "            emit(tags[i].text, 1);\n",
    "    }''',\n",
    "    reduce='''function(key, values) {\n",
    "        return Array.sum(values); \n",
    "    }''',\n",
    "    out='tagsfrequency'\n",
    ")\n",
    "\n",
    "print(list(\n",
    "    db.tagsfrequency.find({'value': {'$gt': 10}})\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the ``map_reduce`` command has now been properly split across the nodes of our cluster. Our shards should report in their logs something like:\n",
    "\n",
    "```\n",
    "2015-06-28T12:31:58.231+0200 I COMMAND  [conn4] command twitter.$cmd command: mapReduce { mapreduce: \"tweets\", map: \"function() {\n",
    "        var tags = this.entities.hashtags;\n",
    "        for(var i=0; i<tags.length; i++)\n",
    "            emit(tags[i].text, 1);\n",
    "    }\", reduce: \"function(key, values) {\n",
    "        return Array.sum(values); \n",
    "    }\", out: \"tmp.mrs.tweets_1435487518_0\", shardedFirstPass: true } ntoreturn:1 keyUpdates:0 writeConflicts:0 numYields:0 reslen:151 locks:{ Global: { acquireCount: { r: 2225, w: 1068, W: 3 } }, MMAPV1Journal: { acquireCount: { r: 575, w: 2130 } }, Database: { acquireCount: { r: 535, w: 1060, R: 42, W: 11 } }, Collection: { acquireCount: { R: 535, W: 1063 } }, Metadata: { acquireCount: { W: 8 } } } 102ms\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performances\n",
    "============\n",
    "\n",
    "Journal Performances\n",
    "--------------------\n",
    "\n",
    "* Write performance is reduced by 5-30%\n",
    "* For apps that are write-heavy (1000+ writes per  server) there can be slowdown due to mix of  journal and data flushes. \n",
    "\n",
    "To avoid Journal Overhead save the journal on a separate DISK from data, it will lower the journal overhead **down to 3%**.\n",
    "\n",
    "Fragmentation\n",
    "-------------\n",
    "\n",
    "Files can get fragmented over time if remove() and update() are issued.\n",
    "* It gets worse if documents have varied sizes\n",
    "* Fragmentation wastes disk space and RAM\n",
    "* Also makes writes scattered and slower (have to lookup for an empty slot in extent)\n",
    "* Fragmentation can be checked by comparing size to storageSize in the collection’s stats\n",
    "* nmoved=1 in logs means document has been resized and moved to another extent\n",
    "\n",
    "**PowerOf2Allocation** is default on 2.6, is more efficient in case of updates/remove as each record has a size in bytes that is a power of 2 (e.g. 32, 64, 128, 256, 512...) so when updating documents they probably have not need to be moved (if document was 200bytes it will have up to 56 more bytes before needing to be reallocated) and when deleted it will leave a slot that can be reused for another document as it will match for sure the same size being rounded to powers of 2.\n",
    "\n",
    "https://github.com/10gen-labs/storage-viz helps debugging storage, RAM and fragmentation.\n",
    "\n",
    "Replication Lag\n",
    "---------------\n",
    "\n",
    "* Secondaries underspec’d vs primaries \n",
    "* Access patterns between primary and secondaries \n",
    "* Insufficient bandwidth (Estimate required bandwidth to sync: ``op/sec * docsize + 40%``)\n",
    "* Foreground index builds on secondaries\n",
    "\n",
    "https://github.com/rueckstiess/mtools helps debugging operations logs and slow replication\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

